[global]

    filestore_fd_cache_size = 204800
    filestore_omap_header_cache_size = 204800
    filestore_xattr_use_omap = true

    osd pool default pg num = 512
    osd pool default pgp num = 512
    osd pool default size = 3
    osd pool default min size = 2

    ### MUST Configure network value used by ceph
    public network = 172.16.0.0/24
    cluster network = 172.16.0.0/24

    ## allow ourselves to open a lot of files
    max open files = 320000

    ## osds check heartbeats
    # osd_heartbeat_interval : interval to check heartbeat of other OSD Daemon
    #          Default value : 6 secs
    # osd_heartbeat_grace    : osd grace period to mark OSD Down
    #          Default value : 20 secs
    # So, osd will report another OSD Down every 'osd_heartbeat_grace' secs
    osd_heartbeat_interval = 10
    osd_heartbeat_grace = 60

    ## osds report down osds
    # mon_osd_min_down_reports   : minmum number of 'OSD Down' reports
    #              Default value : 3 times
    # mon_osd_min_down_reporters : minmum number of osds required to report a OSD Down
    #              Default value : 1
    mon_osd_min_down_reports = 5
    mon_osd_min_down_reporters = 3

    ## osds report there status
    # mon_osd_report_timeout    : Secs before Monitor consider OSD Down
    # mon_osd_down_out_interval : Secs before Monitor consider OSD Down and Out
    mon_osd_report_timeout = 800
    mon_osd_down_out_interval = 1000

    log_to_syslog = false        ; uncomment this line to log to syslog
    ms_nocrc = true

    filestore_fiemap = true

##################
## Monitors
## You need at least one. You need at least three if you want to
## tolerate any node failures. Always create an odd number.
[mon]
    hosts = cloud01,cloud02, cloud03

    mon clock drift allowed    = .15
    mon_pg_warn_max_object_skew = 0
    mon osd full ratio = 0.90
    mon osd nearfull ratio = 0.80

[osd]
    hosts = cloud01, cloud02, cloud03, cloud04

    # Example configuration use file system path
    #osd.cloud01.paths = /var/lib/ceph/osd/ceph-0, /var/lib/ceph/osd/ceph-1
    #osd.cloud02.paths = /var/lib/ceph/osd/ceph-2, /var/lib/ceph/osd/ceph-3
    # Configuration with host devices
    osd.cloud01.devs = /dev/sdb:sdl, /dev/sdc:sdl, /dev/sdd:sdl, /dev/sde:sdl, /dev/sdf:sdl, /dev/sdg:sdm, /dev/sdh:sdm, /dev/sdi:sdm, /dev/sdj:sdm, /dev/sdk:sdm
    osd.cloud02.devs = /dev/sdb:sdl, /dev/sdc:sdl, /dev/sdd:sdl, /dev/sde:sdl, /dev/sdf:sdl, /dev/sdg:sdm, /dev/sdh:sdm, /dev/sdi:sdm, /dev/sdj:sdm, /dev/sdk:sdm
    osd.cloud03.devs = /dev/sdb:sdl, /dev/sdc:sdl, /dev/sdd:sdl, /dev/sde:sdl, /dev/sdf:sdl, /dev/sdg:sdm, /dev/sdh:sdm, /dev/sdi:sdm, /dev/sdj:sdm, /dev/sdk:sdm
    osd.cloud04.devs = /dev/sdb:sdl, /dev/sdc:sdl, /dev/sdd:sdl, /dev/sde:sdl, /dev/sdf:sdl, /dev/sdg:sdm, /dev/sdh:sdm, /dev/sdi:sdm, /dev/sdj:sdm, /dev/sdk:sdm

    # osd journal = /osd-journal-path/osd$id/journal
    osd journal size = 20480    ;journal size, in megabytes

    osd_recovery_threads = 1
    osd recovery max active = 2
    osd_max_backfills = 2
    osd_recovery_max_chunk = 4194304
    osd_recovery_op_priority = 5

    osd mkfs type = xfs
    osd mount options xfs = rw,noatime,inode64,logbsize=256k,delaylog
    osd mkfs options xfs = -f -i size=2048

    osd op threads = 4
    osd disk threads = 2
    osd_map_cache_size = 1024
    osd_op_num_threads_per_shard = 2
    osd_op_num_shards = 15

    osd_op_thread_timeout = 5
    osd op history size = 20

    filestore queue max ops = 50000
    filestore queue max bytes = 10485760000
    filestore queue committing max bytes = 10485760000
    filestore queue committing max ops = 50000
    filestore op threads = 8
    filestore journal writeahead = true

    ## filesytem config
    filestore_wbthrottle_xfs_ios_start_flusher = 10000000
    filestore_wbthrottle_xfs_ios_hard_limit = 10000000
    filestore_wbthrottle_xfs_inodes_start_flusher = 10000
    filestore_wbthrottle_xfs_inodes_hard_limit = 10000
    filestore_wbthrottle_xfs_bytes_start_flusher = 419430400
    filestore_wbthrottle_xfs_bytes_hard_limit = 4194304000

    journal max write entries = 50000
    journal queue max ops = 50000
    objecter inflight ops = 819200
    journal max write bytes = 10485760000
    journal queue max bytes = 10485760000
    ms dispatch throttle bytes = 1048576000
    objecter inflight op bytes = 1048576000
    osd_client_message_cap = 1000000
    osd_client_message_size_cap = 2147483648
    osd_max_write_size = 512

    ## manage the CRUSH map entirely manually
    ## only need configure this after we had set our crushmap
    #osd crush update on start = false


[client]
    rbd cache = true
    rbd cache size = 67108864
    rbd cache max dirty = 33554432
    rbd cache max dirty age = 5
    rbd cache writethrough until flush = true
    rbd_default_order = 25
    rbd_default_stripe_unit = 4194304
    rbd_default_stripe_count = 8
    admin socket = /var/run/ceph/ceph-client/$cluster-$type.$id.$pid.asok
    log file = /var/run/ceph/ceph-client/$cluster-$name.log

    [client.images]
        keyring = /etc/ceph/images.keyring
    [client.volumes]
        keyring = /etc/ceph/volumes.keyring
    [client.cinder]
        keyring = /etc/ceph/cinder.keyring

