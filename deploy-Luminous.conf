[global]
osd_pool_default_size = 2
mon_max_pg_per_osd = 300

[mon]
mon_osd_min_down_reporters = 5

[osd]
bluefs_buffered_io = true
bluestore_csum_type = none

bluestore_rocksdb_options = compression=kNoCompression,max_write_buffer_number=64,min_write_buffer_number_to_merge=32,recycle_log_file_num=64,compaction_style=kCompactionStyleLevel,write_buffer_size=4MB,target_file_size_base=4MB,max_background_compactions=64,level0_file_num_compaction_trigger=64,level0_slowdown_writes_trigger=128,level0_stop_writes_trigger=256,max_bytes_for_level_base=6GB,compaction_threads=32,flusher_threads=8,compaction_readahead_size=2MB

##################
## host-specific
## Put your settings for each node here, these settings will be removed
## from the ultimate ceph.conf
[host-specific]
mon_hosts = ceph0, ceph1, ceph2
osd_hosts = ceph0, ceph1, ceph2
mds_hosts = ceph0, ceph1, ceph2
# Now we don't need Ceph RadosGW
#rgw_hosts = ceph0, ceph1, ceph2

## Please modify below configurations according to your devices used for OSDs
## Three example configurations about OSD with devices
# Use whole disk
osd.ceph0.devs = /dev/sdc
# Use whole disk partition
osd.ceph1.devs = /dev/sdc1
# Specify bluestore block.db and block.wal device
osd.ceph2.devs = /dev/sdc:/dev/sdb1:/dev/sdb2

## for multi network settings
## public network 1;cluster network1 = Host1, Host2, Host3
## public network 2;cluster network2 = Host4, Host5
172.20.4.0/24;172.20.4.0/24 = ceph0, ceph1, ceph2

## Configurations about Ceph crush/pools/cephfs/...
# crush rule format - rules = <class1>, <class2>, ...
[crush]
rules = hdd, ssd

# pools format - <purpose> = <pool_name>:<crush_rule>, ...
[pools]
rbd = replicatedpool:hdd

# one cephfs need configure below four parameters
# mdata_pool - cephfs metadata pool, Only support ONE
# data_pools - cephfs data pools, support multi pools
[cephfs]
name = myfs
mdata_pool = cephfs-mdata:ssd
data_pools = cephfs-data:hdd, cephfs-data-ssd:ssd
max_mds = 2
